"""
Multi-Task MIL with Contrastive Loss + Bayesian Optimization + 3-Fold CV using Lazy Loading and Mixed Precision Training

This script implements a multi-task MIL model that uses pre-computed embeddings 
(for example, tensors loaded from .pt files) for binary prediction of a target cancer while leveraging auxiliary data 
(cancer type) from other cancers. The auxiliary cancer type is provided as a string (e.g. "prostate", "breast", etc.). 
A global mapping is computed to convert these strings into integer labels, and samples with unknown or missing 
cancer types are discarded.

Key features:
  - 3-fold cross validation. Each fold (fold-0, fold-1, fold-2) contains CSV files for train, tune (validation) and test labels.
  - Modular data loading: the CSV files contain at least:
      • a case_id (used to match embedding filenames)
      • a primary (binary) label (default column name: "label")
      • an auxiliary label (cancer type as a string; default column name: "cancer_type")
  - Lazy loading is used: the dataset stores embedding file paths (instead of loading entire embeddings upfront) and loads them on demand.
  - Multi-task model with:
      • Shared feature extraction (an MLP) applied to each embedding.
      • An attention (or gated attention) mechanism to aggregate patch features into a bag representation.
      • A primary head for binary classification and an auxiliary head for cancer type classification.
      • A supervised contrastive loss computed on the bag representations.
  - Bayesian optimization (Optuna with multi-fidelity pruning) is used to tune hyperparameters.
  - Mixed precision training is enabled via torch.cuda.amp.
  - Extensive logging via the logging module and wandb integration.
  - ROC curve plotting and ROC-AUC measurement via scikit-learn and matplotlib.

Requirements:
  - PyTorch, pandas, numpy
  - optuna, wandb
  - scikit-learn, matplotlib
"""

import argparse
import os
import sys
import glob
import logging
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import wandb
from collections import Counter
from sklearn.metrics import roc_auc_score, roc_curve
from torch.cuda.amp import GradScaler, autocast
import re
from typing import List, Dict
import os, psutil
import h5py
from PIL import Image
from openslide import OpenSlide
import matplotlib.pyplot as plt
import matplotlib.cm as cm# For mixed precision training


best_auc_global = 0.0          # tracks best avg-test AUC achieved so far
best_model_path = "/data/temporary/amirhosein/UNI_processed/best_model_3cancer_baseline.pth"
# -------------------------
# Setup Logging
# -------------------------
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# -------------------------
# Data Loading Utilities (Lazy Loading)
# -------------------------
def compute_aux_mapping(fold_dir, aux_label_col):
    """
    Computes a global auxiliary label mapping dictionary from all CSV files across folds.
    Reads all CSVs from fold-0, fold-1, and fold-2 to collect valid cancer type strings 
    (converted to lower-case) and assigns a unique integer to each.
    Samples with "unknown", empty, or "nan" values are discarded.
    """
    aux_values = set()
    for fold in range(3):
        for csv_name in ["train_updated.csv", "tune_updated.csv", "test_updated.csv"]:
            csv_path = os.path.join(fold_dir, f"fold-{fold}", csv_name)
            if not os.path.exists(csv_path):
                continue
            df = pd.read_csv(csv_path)
            valid_aux = df[aux_label_col].astype(str).str.strip().str.lower()
            valid_aux = valid_aux[~valid_aux.isin(["unknown"])]
            aux_values.update(valid_aux.unique())
    mapping = {val: idx for idx, val in enumerate(sorted(aux_values))}
    logger.info("Computed auxiliary mapping:")
    for key, value in mapping.items():
        logger.info(f"    {key}: {value}")
    return mapping

def build_dataset_info(label_file, embeddings_dir, label_col="TP53", case_id_col="case_id", 
                       aux_label_col=None, aux_mapping=None):
    """
    Reads a CSV file (without loading the embeddings) to build a list of tuples.
    Each tuple contains:
       (embedding_file_path, primary_label)  
       OR  
       (embedding_file_path, primary_label, aux_label)
    based on whether aux_label_col is provided.
    Samples with missing embeddings or invalid auxiliary labels are discarded.
    
    Additionally, if aux_label_col is provided, logs the distribution of auxiliary labels.
    """
    logger.info(f"Loading label info from: {label_file}")
    df = pd.read_csv(label_file)
    info_list = []
    skipped_due_to_aux = 0
    for idx, row in df.iterrows():
        case_id = str(row[case_id_col])
        primary_label = row[label_col]
        if aux_label_col is not None:
            aux_raw = row[aux_label_col]
            aux_str = str(aux_raw).strip().lower()
            #if aux_str in ["unknown"]:
            #    skipped_due_to_aux += 1
            #    continue
            if aux_mapping is not None:
                if aux_str not in aux_mapping:
                    logger.warning(f"Auxiliary label '{aux_str}' not in mapping; skipping case_id {case_id}.")
                    continue
                aux_label = aux_mapping[aux_str]
            else:
                Print("No auxiliary mapping provided. Using raw label.")
                try:
                    aux_label = int(aux_raw)
                except ValueError:
                    logger.warning(f"Invalid auxiliary label '{aux_raw}' for case_id {case_id}; skipping.")
                    skipped_due_to_aux += 1
                    continue
        pattern = os.path.join(embeddings_dir, f"*{case_id}*.pt")
        files = glob.glob(pattern)
        if not files:
            logger.warning(f"No embedding file found for case_id: {case_id}. Skipping sample.")
            continue
        file_path = files[0]
        if aux_label_col is not None:
            info_list.append((file_path, primary_label, int(aux_label)))
        else:
            info_list.append((file_path, primary_label))
    logger.info(f"Built dataset info for {len(info_list)} samples from {label_file}. Skipped {skipped_due_to_aux} samples due to invalid auxiliary labels.")

    # If auxiliary labels are available, log their distribution.
    if aux_label_col is not None and len(info_list) > 0:
        aux_labels = [sample[2] for sample in info_list]
        aux_counts = Counter(aux_labels)
        # Optionally, convert back to the original cancer type strings using an inverse mapping:
        inv_mapping = {v: k for k, v in aux_mapping.items()} if aux_mapping is not None else {}
        distribution_str = ", ".join([f"{inv_mapping.get(label, label)}: {count}" for label, count in aux_counts.items()])
        logger.info(f"Auxiliary label distribution for {label_file}: {distribution_str}")

    return info_list

class LazyEmbeddingsDataset(data.Dataset):
    """
    PyTorch Dataset for MIL using lazy loading.
    Instead of loading the embedding tensor at initialization,
    only the file path (and associated labels) is stored.
    The embedding is loaded from disk on each __getitem__ call.
    Each sample is a tuple:
         (embedding, primary_label, aux_label)  OR
         (embedding, primary_label)
    """
    def __init__(self, info_list):
        self.info_list = info_list

    def __len__(self):
        return len(self.info_list)

    def __getitem__(self, index):
        # Retrieve the stored file path and labels
        sample = self.info_list[index]
        file_path = sample[0]
        try:
            embedding = torch.load(file_path)
        except Exception as e:
            logger.warning(f"Error loading embedding from {file_path}: {e}")
            # In the event of error, return a dummy tensor (adjust as appropriate)
            embedding = torch.zeros((1,))  
        if len(sample) == 3:
            return embedding, sample[1], sample[2]
        else:
            return embedding, sample[1]

def collate_fn(batch):
    """
    Filter out any None items. Usually not needed if you skip them beforehand,
    but this is an extra safeguard in case something still yields None.
    """
    valid_items = []
    for item in batch:
        if item is not None:
            valid_items.append(item)
        else:
            logging.warning("Skipped a None item during collate_fn.")
    return valid_items


# -------------------------
# Supervised Contrastive Loss
# -------------------------
def supervised_contrastive_loss(features, labels, temperature=0.07):
    """
    Compute a supervised contrastive loss given a batch of features and integer labels.
    features: tensor of shape [batch_size, feature_dim]
    labels: tensor of shape [batch_size] of integers.
    Encourages features from samples with the same label to cluster together, 
    while pushing apart features with different labels.
    """
    if features.size(0) < 2:
        return torch.tensor(0.0, device=features.device)
    
    features = nn.functional.normalize(features, p=2, dim=1)
    similarity_matrix = torch.matmul(features, features.T) / temperature
    
    batch_size = features.size(0)
    mask = torch.eye(batch_size, device=features.device).bool()
    
    loss = 0.0
    count = 0
    for i in range(batch_size):
        pos_mask = (labels == labels[i])
        pos_mask[i] = False
        if pos_mask.sum().item() == 0:
            continue
        exp_sim = torch.exp(similarity_matrix[i][~mask[i]])
        denominator = exp_sim.sum()
        numerator = torch.exp(similarity_matrix[i][pos_mask]).sum()
        loss_i = -torch.log((numerator + 1e-8) / (denominator + 1e-8))
        loss += loss_i
        count += 1
    return loss / count if count > 0 else torch.tensor(0.0, device=features.device)

# -------------------------
# Model Definition: Multi-Task MIL with Contrastive Loss
# -------------------------
class MultiTaskMILClassifier(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=128, M=500, L=128, 
                 attention_type="attention", aux_num_classes=3):
        """
        Multi-task MIL model.
        Each bag is processed by:
          • A shared MLP feature extractor (applied patchwise).
          • A projection to an M-dimensional space.
          • An attention (or gated attention) mechanism to aggregate patch features into a bag-level representation.
        Two classifier heads are used:
          • A primary head for binary classification.
          • An auxiliary head for cancer type classification.
        The bag representation is also used to compute a supervised contrastive loss.
        """
        super(MultiTaskMILClassifier, self).__init__()
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, M),
            nn.ReLU()
        )
        self.M = M
        self.L = L
        self.attention_type = attention_type
        
        if attention_type == "attention":
            self.attention_layer = nn.Sequential(
                nn.Linear(M, L),
                nn.Tanh(),
                nn.Linear(L, 1)
            )
        elif attention_type == "gated_attention":
            self.attention_V = nn.Sequential(
                nn.Linear(M, L),
                nn.Tanh()
            )
            self.attention_U = nn.Sequential(
                nn.Linear(M, L),
                nn.Sigmoid()
            )
            self.attention_layer = nn.Linear(L, 1)
        else:
            raise ValueError("Invalid attention_type. Choose 'attention' or 'gated_attention'.")
        
        self.classifier_head = nn.Linear(M, 1)   # Primary (binary) classifier head
        self.classifier_aux = nn.Linear(M, aux_num_classes)  # Auxiliary classifier head
        
    def forward(self, x):
        """
        Forward pass for a single bag.
        x: tensor of shape [N, input_dim] (where N is the number of patches).
        Returns:
          bag_rep: Aggregated bag-level representation (1 x M) then squeezed.
          binary_logit: Primary output (scalar logit).
          aux_logits: Auxiliary output (logits for cancer type classification).
          attention_weights: Attention weights (1 x N).
        """
        H = self.feature_extractor(x)        # [N, hidden_dim]
        H_proj = self.projection(H)            # [N, M]
        
        if self.attention_type == "attention":
            A = self.attention_layer(H_proj)   # [N, 1]
        else:
            A_V = self.attention_V(H_proj)       # [N, L]
            A_U = self.attention_U(H_proj)       # [N, L]
            A = self.attention_layer(A_V * A_U)  # [N, 1]
        
        A = torch.transpose(A, 0, 1)           # [1, N]
        A = torch.softmax(A, dim=1)            # [1, N]
        bag_rep = torch.mm(A, H_proj)          # [1, M]
        
        binary_logit = self.classifier_head(bag_rep)  # [1, 1]
        aux_logits = self.classifier_aux(bag_rep)       # [1, aux_num_classes]
        
        binary_logit = torch.squeeze(binary_logit)  # scalar
        bag_rep = torch.squeeze(bag_rep)            # [M]
        
        return bag_rep, binary_logit, aux_logits, A

# -------------------------
# Training and Evaluation Functions
# -------------------------
def evaluate_model(model, data_loader, device):
    """
    Evaluate the model on a dataset and compute ROC-AUC for the primary task.
    """
    model.eval()
    all_labels = []
    all_probs = []
    with torch.no_grad():
        for batch in data_loader:
            for sample in batch:
                if len(sample) == 3:
                    embedding, primary_label, _ = sample
                else:
                    embedding, primary_label = sample
                embedding = embedding.to(device).float()
                _, binary_logit, _, _ = model(embedding)
                prob = torch.sigmoid(binary_logit).item()
                all_probs.append(prob)
                all_labels.append(float(primary_label))
    try:
        auc = roc_auc_score(all_labels, all_probs)
    except Exception as e:
        logger.warning("ROC AUC calculation failed: " + str(e))
        auc = 0.0
    return auc

def plot_roc(all_labels, all_probs, output_path):
    """
    Plot the ROC curve using matplotlib and save the figure.
    """
    fpr, tpr, _ = roc_curve(all_labels, all_probs)
    plt.figure()
    plt.plot(fpr, tpr, label="ROC curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend(loc="best")
    plt.savefig(output_path)
    plt.close()
    logger.info(f"ROC curve saved to {output_path}")

def train_fold(model, train_loader, tune_loader, optimizer, device, epochs, 
               primary_weight=1.0, aux_weight=1.0, contrast_weight=0.5, trial=None):
    """
    Train the model on the training set and evaluate on the tune set.
    Computes:
      - Primary loss: BCEWithLogitsLoss for binary classification.
      - Auxiliary loss: CrossEntropyLoss for cancer type classification.
      - Contrastive loss on bag-level representations (using auxiliary labels).
    Total loss is the weighted sum of these losses.
    Uses mixed precision training via torch.cuda.amp.
    Intermediate validation ROC-AUC is reported to the trial (if provided).
    """
    model.train()
    primary_loss_fn = nn.BCEWithLogitsLoss()
    aux_loss_fn = nn.CrossEntropyLoss()
    best_val_auc = 0.0
    scaler = GradScaler(device='cuda')  # For mixed precision
    
    for epoch in range(epochs):
        running_loss = 0.0
        running_contrast_loss = 0.0
        num_batches = 0
        for batch in train_loader:
            batch_aux_labels = []
            bag_reps = []
            batch_primary_labels_int = [] 
            batch_loss = 0.0
            optimizer.zero_grad()
            with autocast(device_type='cuda'):  # Mixed precision forward pass
                for sample in batch:
                    if len(sample) == 3:
                        embedding, primary_label, aux_label = sample
                    else:
                        embedding, primary_label = sample
                        aux_label = 0
                    embedding = embedding.to(device).float()
                    bag_rep, binary_logit, aux_logits, _ = model(embedding)
                    batch_primary_labels_int.append(torch.tensor([int(primary_label)], device=device))

                    batch_aux_labels.append(torch.tensor([int(aux_label)], device=device))
                    bag_reps.append(bag_rep.unsqueeze(0))
                    primary_loss = primary_loss_fn(binary_logit, torch.tensor(float(primary_label), device=device))
                    try:
                        aux_val = int(aux_label)
                    except (ValueError, TypeError):
                        logger.warning(f"Invalid auxiliary label '{aux_label}' encountered; setting default value 0.")
                        aux_val = 0
                    aux_label_tensor = torch.tensor([int(aux_label)], device=device)  # shape [1]
                    aux_loss = aux_loss_fn(aux_logits, aux_label_tensor)
                    sample_loss = primary_weight * primary_loss + aux_weight * aux_loss
                    batch_loss += sample_loss
                if len(bag_reps) > 1:
                    bag_reps_tensor = torch.cat(bag_reps, dim=0)  # [batch_size, M]
                    aux_labels_tensor = torch.cat(batch_aux_labels, dim=0)
                    primary_labels_tensor = torch.cat(batch_primary_labels_int, dim=0)
                    contrast_loss = supervised_contrastive_loss(bag_reps_tensor, primary_labels_tensor)
                else:
                    contrast_loss = torch.tensor(0.0, device=device)
                total_loss = batch_loss + contrast_weight * contrast_loss
            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            running_loss += total_loss.item()
            running_contrast_loss += contrast_loss.item()
            num_batches += 1
        avg_loss = running_loss / num_batches if num_batches else 0.0
        val_auc = evaluate_model(model, tune_loader, device)
        logger.info(f"Epoch {epoch+1}/{epochs}: Avg Loss: {avg_loss:.4f}, Tune ROC-AUC: {val_auc:.4f}")
        wandb.log({"epoch": epoch+1, "train_loss": avg_loss, "tune_auc": val_auc, 
                   "contrast_loss": running_contrast_loss / num_batches})
        if trial is not None:
            trial.report(val_auc, epoch)
            if trial.should_prune():
                logger.info(f"Trial pruned at epoch {epoch+1}")
                raise optuna.exceptions.TrialPruned()
        if val_auc > best_val_auc:
            best_val_auc = val_auc
    return best_val_auc

# ---------------------------------------------------------------------------- #
#                                    Config                                   #
# ---------------------------------------------------------------------------- #
MODELS: List[Dict] = [
    {
        "name": "model 6 cancer",
        "checkpoint": "/data/temporary/amirhosein/UNI_processed/best_model_6cancer_baseline.pth",
    },
    {
        "name": "3 cancer",
        "checkpoint": "/data/temporary/amirhosein/UNI_processed/best_model_3cancer_contrast.pth",
    },
    {
        "name": "prostate",
        "checkpoint": "/data/temporary/amirhosein/UNI_processed/best_model_prostate.pth",
    },
    # Add more models here...
]

# ---------------------------------------------------------------------------- #
#                              Helper Functions                                #
# ---------------------------------------------------------------------------- #

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)8s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

_proc = psutil.Process(os.getpid())
def log_mem(label=""):
    # CPU RSS in MiB
    rss = _proc.memory_info().rss / 1024**2
    # (optionally) GPU allocated in MiB
    if torch.cuda.is_available():
        gpu = torch.cuda.memory_allocated() / 1024**2
        logging.info(f"[MEM] {label} — CPU: {rss:.1f} MiB, GPU: {gpu:.1f} MiB")
    else:
        logging.info(f"[MEM] {label} — CPU: {rss:.1f} MiB")

def parse_args(cli_args=None):
    p = argparse.ArgumentParser(
        description="Generate side‑by‑side original vs. attention‑heatmap thumbnails"
    )
    p.add_argument("--test_csv",    default="/data/temporary/amirhosein/UNI_processed/labels/3-folds-prostate-only/fold-1/test_updated.csv", help="CSV with case_id,TP53 columns")
    p.add_argument("--clam_h5_dir", default="/data/temporary/nadieh/patches_uni/patches", help="Directory of CLAM .h5 coords files")
    p.add_argument("--uni_emb_dir", default="/data/temporary/amirhosein/UNI_processed/model_script_concatenated", help="Directory of UNI .pt embedding files")
    p.add_argument("--slide_root",  default="/data/pa_cpgarchive/archives/prostate/tcga/images/Diagnostic_image", help="Root directory for WSIs (recursive search)")
    p.add_argument("--output_dir",  default="/data/temporary/amirhosein/UNI_processed/outputs", help="Where to save comparison images")
    p.add_argument("--pos_n",       type=int, default=1, help="Number of TP53=1 samples")
    p.add_argument("--neg_n",       type=int, default=1, help="Number of TP53=0 samples")
    p.add_argument("--patch_size",  type=int, default=256, help="Patch size used by CLAM")
    p.add_argument("--downsample",  type=int, default=16,  help="Downsample factor for thumbnails")
    p.add_argument("--cmap",        default="inferno", help="Matplotlib colormap name")
    p.add_argument("--alpha",       type=float, default=0.8, help="Overlay transparency")
    p.add_argument("--clip_lo",     type=float, default=5,   help="Lower percentile clamp")
    p.add_argument("--clip_hi",     type=float, default=99,  help="Upper percentile clamp")
    p.add_argument("--gamma",       type=float, default=0.5, help="Gamma correction")
    p.add_argument("--diff_base",  help="Model name to use as the *base* in the difference map")
    p.add_argument("--diff_other", help="Model name to subtract from the base")
    args, unknown = p.parse_known_args(cli_args)
    if unknown:
        logging.warning(f"Ignoring unknown args: {unknown}")
    return args


# ------------------------- Path‑resolution utilities ------------------------- #

def find_path_by_prefix(case_id: str, paths: List[str]) -> str:
    """Return the best path whose *basename* starts with ``case_id`` (case‑insensitive).

    If multiple matches exist, pick the one with the lowest DX number (e.g. ``DX1``).
    """
    cid_lower = case_id.lower()
    matches = [p for p in paths if os.path.basename(p).lower().startswith(cid_lower)]
    if not matches:
        return ""

    # Rank by DX number (if present) – smaller is better.
    def dx_rank(p: str) -> int:
        m = re.search(r"-DX(\d+)", os.path.basename(p), re.IGNORECASE)
        return int(m.group(1)) if m else 999

    matches.sort(key=dx_rank)
    chosen = matches[0]
    if len(matches) > 1:
        logging.warning(
            f"Multiple matches for case_id '{case_id}'. Using '{os.path.basename(chosen)}'. Other candidates: {matches}"
        )
    return chosen

# ---------------------------------------------------------------------------- #
#                           Attention‑map helpers                               #
# ---------------------------------------------------------------------------- #
# NOTE: replace `MultiTaskMILClassifier` import path below with your own impl.


def load_model_auto(checkpoint_path: str, device: torch.device) -> MultiTaskMILClassifier:
    state = torch.load(checkpoint_path, map_location=device)

    # Infer layer sizes automatically from checkpoint weights
    fe_w   = state["feature_extractor.0.weight"]
    hd_dim, in_dim = fe_w.shape
    proj_w = state["projection.0.weight"]
    M_dim, _       = proj_w.shape
    aux_w  = state["classifier_aux.weight"]
    aux_cls, _     = aux_w.shape
    att_w  = state["attention_layer.0.weight"]
    L_dim, _       = att_w.shape

    model = MultiTaskMILClassifier(
        input_dim=in_dim,
        hidden_dim=hd_dim,
        M=M_dim,
        L=L_dim,
        attention_type="attention",
        aux_num_classes=aux_cls,
    )
    model.load_state_dict(state, strict=False)
    model.to(device).eval()
    return model


def get_attention_weights(coords: np.ndarray, embeds: torch.Tensor, model: MultiTaskMILClassifier, device: torch.device) -> np.ndarray:
    with torch.no_grad():
        _, _, _, A = model(embeds.to(device).float())
    return A.squeeze(0).cpu().numpy()


def build_canvas(
    coords: np.ndarray,
    attn: np.ndarray,
    height: int,
    width: int,
    patch: int,
    downsample: int
) -> np.ndarray:
    """
    Builds an attention canvas at thumbnail resolution rather than full WSI size.

    - coords are original patch coordinates on the WSI.
    - downsample is the same factor used when generating the thumbnail.
    """
    # Compute thumbnail dimensions and patch size in downsampled space
    tw, th = width // downsample, height // downsample
    patch_ds = max(1, patch // downsample)

    canvas = np.zeros((th, tw), dtype=np.float32)
    counts = np.zeros((th, tw), dtype=np.uint16)

    for (x, y), w in zip(coords, attn):
        # Map WSI coords into thumbnail grid
        x0 = x // downsample
        y0 = y // downsample
        # Clamp to valid range
        x1 = min(tw, x0 + patch_ds)
        y1 = min(th, y0 + patch_ds)

        canvas[y0:y1, x0:x1] += w
        counts[y0:y1, x0:x1] += 1

    counts[counts == 0] = 1
    return canvas / counts


def colorize(canvas: np.ndarray, lo: float, hi: float, gamma: float, cmap_name: str) -> np.ndarray:
    norm = (canvas - canvas.min()) / (canvas.max() - canvas.min() + 1e-8)
    lo_v, hi_v = np.percentile(norm, [lo, hi])
    clipped = np.clip(norm, lo_v, hi_v)
    adj = ((clipped - lo_v) / (hi_v - lo_v + 1e-8)) ** gamma
    cmap = cm.get_cmap(cmap_name)
    return cmap(adj)[..., :3]  # RGB (0‑1)

# ---------------------------------------------------------------------------- #
#                                      Main                                   #
# ---------------------------------------------------------------------------- #

def main():
    setup_logging()
    args = parse_args()

    # --- Enumerate files once -------------------------------------------------
    logging.info("Scanning file system …")
    clam_h5_paths = glob.glob(os.path.join(args.clam_h5_dir, "**", "*.h5"), recursive=True)
    uni_pt_paths  = glob.glob(os.path.join(args.uni_emb_dir, "**", "*.pt"), recursive=True)
    slide_paths   = glob.glob(os.path.join(args.slide_root, "**", "*.svs"), recursive=True)

    # --- Build DataFrame ------------------------------------------------------
    logging.info("Reading label CSV …")
    df = pd.read_csv(args.test_csv, dtype={"case_id": str, "TP53": int})

    logging.info("Resolving paths by *prefix* match …")
    df["h5_path"]    = df.case_id.apply(lambda cid: find_path_by_prefix(cid, clam_h5_paths))
    df["pt_path"]    = df.case_id.apply(lambda cid: find_path_by_prefix(cid, uni_pt_paths))
    df["slide_path"] = df.case_id.apply(lambda cid: find_path_by_prefix(cid, slide_paths))

    df = df[(df.h5_path != "") & (df.pt_path != "") & (df.slide_path != "")]
    logging.info(f"Resolved paths for {len(df)} slides.")

    # --- Sampling -------------------------------------------------------------
    pos_df = df[df.TP53 == 1]
    neg_df = df[df.TP53 == 0]
    pos = pos_df.sample(min(args.pos_n, len(pos_df)), random_state=42)
    neg = neg_df.sample(min(args.neg_n, len(neg_df)), random_state=42)
    samples = pd.concat([pos, neg]).reset_index(drop=True)
    logging.info(f"Selected {len(samples)} slides – {len(pos)} positive, {len(neg)} negative.")

    # --- Device ---------------------------------------------------------------
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --- Iterate over models --------------------------------------------------
    for mdl in MODELS:
        name, ckpt = mdl["name"], mdl["checkpoint"]
        logging.info(f"\n=== Processing with model '{name}' ===")
        model = load_model_auto(ckpt, device)

        out_dir = os.path.join(args.output_dir, name)
        os.makedirs(out_dir, exist_ok=True)

        for i, row in samples.iterrows():
            cid = row.case_id
            log_mem(f"start {cid}")
            cid, label = row.case_id, row.TP53
            slide_p = row.slide_path
            h5_p    = row.h5_path
            pt_p    = row.pt_path

            if not all([slide_p, h5_p, pt_p]):
                logging.warning(f"Missing files for {cid}; skipping.")
                continue

            logging.info(f"[{i+1}/{len(samples)}] {cid} – TP53={label} – slide: {os.path.basename(slide_p)}")

            # --- Load data -------------------------------------------------
            coords = h5py.File(h5_p, "r")["coords"][:]
            embeds = torch.load(pt_p, map_location="cpu")
            attn   = get_attention_weights(coords, embeds, model, device)

            slide  = OpenSlide(slide_p)
            W, H   = slide.dimensions
            canvas = build_canvas(coords, attn, H, W, args.patch_size, args.downsample)

            # --- Visualization -------------------------------------------
            heat_rgb = colorize(canvas, args.clip_lo, args.clip_hi, args.gamma, args.cmap)

            tw, th = W // args.downsample, H // args.downsample
            thumb  = slide.get_thumbnail((tw, th))
            wsi_np = np.asarray(thumb) / 255.0

            heat_np = np.asarray(Image.fromarray((heat_rgb*255).astype(np.uint8)).resize((tw, th), Image.BILINEAR)) / 255.0
            overlay = args.alpha * heat_np + (1 - args.alpha) * wsi_np

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
            ax1.imshow(wsi_np);  ax1.set_title("Original"); ax1.axis("off")
            ax2.imshow(overlay); ax2.set_title("Attention overlay"); ax2.axis("off")
            fig.suptitle(f"{cid} – TP53={label}")
            fig.tight_layout()

            save_path = os.path.join(out_dir, f"{cid}_comparison.png")
            fig.savefig(save_path, dpi=150)
            plt.close(fig)
            logging.info(f"Saved → {save_path}")

    logging.info("All models finished. ✅")


if __name__ == "__main__":
    main()
